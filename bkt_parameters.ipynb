{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read data\n",
    "data needs to be sorted by skill, student id, time\n",
    "sort if not sorted already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes as ct\n",
    "import sklearn.metrics as sk\n",
    "import random\n",
    "import statistics\n",
    "\n",
    "df = pd.read_excel('/Users/nabilch/Desktop/bkt_c/rori_rising_data_for_bkt_microlesson.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get parameters for each skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parmtrs_4_1_skill(temp_df):\n",
    "    size=len(temp_df)\n",
    "    right=(ct.c_int*size)()\n",
    "    student_id=(ct.c_int*size)()\n",
    "    rows=(ct.c_int)()\n",
    "    rows=size\n",
    "    previous_student=\"\"\n",
    "    student_count=0\n",
    "    for i in range(size):\n",
    "        right[i]=temp_df['right'][i]\n",
    "        if(temp_df['student'][i]!=previous_student):\n",
    "            student_count=student_count+1\n",
    "            previous_student=temp_df['student'][i]\n",
    "        student_id[i]=student_count\n",
    "        \n",
    "    libObj.get_bkt_par_c.restype = ct.POINTER(ct.c_float)\n",
    "    parameters=libObj.get_bkt_par_c(student_id, right, rows)\n",
    "    L0_temp=round(parameters[0],3)\n",
    "    T_temp=round(parameters[1],3)\n",
    "    S_temp=round(parameters[2],3)\n",
    "    G_temp=round(parameters[3],3)\n",
    "    temp=libObj.free_memory(parameters)\n",
    "    return L0_temp,T_temp,S_temp,G_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get parameters for all skills\n",
    "only considering skills for which at least 25 students attempted at least 3 questions\n",
    "can turn this check (block of code) off if wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parmtrs_all_skills(df_temp):\n",
    "    all_skills_parmtrs=pd.DataFrame(columns=['skill','L0','T','S','G'])\n",
    "    skills=df_temp.skill.unique()\n",
    "    for s in skills:\n",
    "        df_skill=df_temp[df_temp['skill']==s].reset_index(drop=True)\n",
    "        # checcking if there are at least 25 students with at least 3 qns each\n",
    "        temp=df_skill.student.value_counts()\n",
    "        if (len(temp)<25):\n",
    "            continue\n",
    "        if (temp[24]<3):\n",
    "            continue\n",
    "        L0,T,S,G=get_parmtrs_4_1_skill(df_skill)\n",
    "        all_skills_parmtrs=all_skills_parmtrs._append({'skill':s,'L0':L0,'T':T,'S':S,'G':G},ignore_index=True)\n",
    "    return all_skills_parmtrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get predicted probabilities based on model parameters\n",
    "only for skills that were present in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(temp_df, temp_par):\n",
    "    predictions=temp_df[['student','skill','right']]\n",
    "    # only for skills for which we have bkt parameters\n",
    "    predictions=predictions[predictions['skill'].isin(temp_par['skill'])]\n",
    "    predictions=predictions.merge(temp_par, on='skill', how='left')\n",
    "    previous_student=''\n",
    "    previous_skill=''\n",
    "    for i in range(len(predictions)):\n",
    "        if((predictions['student'][i]==previous_student) and (predictions['skill'][i]==previous_skill)):\n",
    "            predictions.at[i,'PriorL']=PostL\n",
    "        else:\n",
    "            predictions.at[i,'PriorL']=predictions['L0'][i]\n",
    "        \n",
    "        if(predictions['right'][i]==0):\n",
    "            cond_prob=(predictions['PriorL'][i]*predictions['S'][i])/((predictions['PriorL'][i]*predictions['S'][i])+((1-predictions['PriorL'][i])*(1-predictions['G'][i])))\n",
    "        else:\n",
    "            cond_prob=(predictions['PriorL'][i]*(1-predictions['S'][i]))/((predictions['PriorL'][i]*(1-predictions['S'][i]))+((1-predictions['PriorL'][i])*predictions['G'][i]))\n",
    "        \n",
    "        PostL=cond_prob+((1-cond_prob)*predictions['T'][i])\n",
    "\n",
    "        previous_student=predictions['student'][i]\n",
    "        previous_skill=predictions['skill'][i]\n",
    "\n",
    "    predictions['Prob_correct']=(predictions['PriorL']*(1-predictions['S']))+((1-predictions['PriorL'])*predictions['G'])\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get collapsed evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapsed_metrics(temp_predicted_probability):\n",
    "    rmse_c=sk.mean_squared_error(temp_predicted_probability['right'],temp_predicted_probability['Prob_correct'],squared=False)\n",
    "    auc_c=sk.roc_auc_score(temp_predicted_probability['right'],temp_predicted_probability['Prob_correct'])\n",
    "    temp=temp_predicted_probability[((temp_predicted_probability['right']==1) & (temp_predicted_probability['Prob_correct']>=0.5)) | ((temp_predicted_probability['right']==0) & (temp_predicted_probability['Prob_correct']<0.5))]\n",
    "    acc_c=len(temp)/len(temp_predicted_probability)\n",
    "    return rmse_c, auc_c, acc_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get non-collapsed evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_collapsed_metrics(temp_predicted_probability):\n",
    "    student_count=0\n",
    "    auc_nc=0\n",
    "    rmse_nc=0\n",
    "    acc_nc=0\n",
    "    all_students=temp_predicted_probability.student.unique()\n",
    "    num_of_students=len(all_students)\n",
    "    for i in range(num_of_students):\n",
    "        temp_prediction=temp_predicted_probability[temp_predicted_probability['student']==all_students[i]]\n",
    "        try:\n",
    "            auc_nc=auc_nc+sk.roc_auc_score(temp_prediction['right'],temp_prediction['Prob_correct'])\n",
    "            student_count=student_count+1\n",
    "        except ValueError:\n",
    "            pass\n",
    "        rmse_nc=rmse_nc+sk.mean_squared_error(temp_prediction['right'],temp_prediction['Prob_correct'], squared=False)\n",
    "        temp=temp_prediction[((temp_prediction['right']==1) & (temp_prediction['Prob_correct']>=0.5)) | ((temp_prediction['right']==0) & (temp_prediction['Prob_correct']<0.5))]\n",
    "        acc_nc=acc_nc+(len(temp)/len(temp_prediction))\n",
    "\n",
    "    auc_nc=auc_nc/student_count\n",
    "    rmse_nc=rmse_nc/num_of_students\n",
    "    acc_nc=acc_nc/num_of_students\n",
    "\n",
    "    return rmse_nc, auc_nc, acc_nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "student stratified n-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss_nfold_crossV(data,nfold):\n",
    "    fold_rmse_c=[0]*nfold\n",
    "    fold_auc_c=[0]*nfold\n",
    "    fold_acc_c=[0]*nfold\n",
    "    fold_rmse_nc=[0]*nfold\n",
    "    fold_auc_nc=[0]*nfold\n",
    "    fold_acc_nc=[0]*nfold\n",
    "    avg_rmse_c=0\n",
    "    avg_auc_c=0\n",
    "    avg_acc_c=0\n",
    "    avg_rmse_nc=0\n",
    "    avg_auc_nc=0\n",
    "    avg_acc_nc=0\n",
    "    students=data.student.unique()\n",
    "    random.shuffle(students)\n",
    "    num_of_students=len(students)\n",
    "    each_fold=num_of_students//nfold\n",
    "    data=data[data['student'].isin(students[0:nfold*each_fold])]\n",
    "    for i in range(nfold):\n",
    "        testing_data=data[data['student'].isin(students[(i*each_fold):(i+1)*each_fold])]\n",
    "        training_data=data[~data['student'].isin(students[(i*each_fold):(i+1)*each_fold])]\n",
    "        bkt_parmtrs_4_all_skills=get_parmtrs_all_skills(training_data)\n",
    "        if(len(bkt_parmtrs_4_all_skills))==0:\n",
    "            fold_rmse_c[i]=fold_auc_c[i]=fold_acc_c[i]=0\n",
    "            fold_rmse_nc[i]=fold_auc_nc[i]=fold_acc_nc[i]=0\n",
    "        else:\n",
    "            predicted_probability=get_predictions(testing_data, bkt_parmtrs_4_all_skills)\n",
    "            fold_rmse_c[i], fold_auc_c[i], fold_acc_c[i]=collapsed_metrics(predicted_probability)\n",
    "            fold_rmse_nc[i], fold_auc_nc[i], fold_acc_nc[i]=non_collapsed_metrics(predicted_probability)\n",
    "    avg_rmse_c=statistics.mean(fold_rmse_c)\n",
    "    avg_auc_c=statistics.mean(fold_auc_c)\n",
    "    avg_acc_c=statistics.mean(fold_acc_c)\n",
    "    avg_rmse_nc=statistics.mean(fold_rmse_nc)\n",
    "    avg_auc_nc=statistics.mean(fold_auc_nc)\n",
    "    avg_acc_nc=statistics.mean(fold_acc_nc)\n",
    "\n",
    "    colpsd_rmse_auc_acc=[avg_rmse_c, avg_auc_c, avg_acc_c]\n",
    "    non_colpsd_rmse_auc_acc=[avg_rmse_nc, avg_auc_nc, avg_acc_nc]\n",
    "    clpsd_fold_rmse_auc_acc=[fold_rmse_c, fold_auc_c, fold_acc_c]\n",
    "    non_clpsd_fold_rmse_auc_acc=[fold_rmse_nc, fold_auc_nc, fold_acc_nc]\n",
    "\n",
    "    return colpsd_rmse_auc_acc, non_colpsd_rmse_auc_acc, clpsd_fold_rmse_auc_acc, non_clpsd_fold_rmse_auc_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "libObj=ct.cdll.LoadLibrary(\"/Users/nabilch/Desktop/bkt_c/find_par.so\")\n",
    "# c_rmse_auc_acc, nc_rmse_auc_acc, fold_c_rmse_auc_acc, fold_nc_rmse_auc_acc=ss_nfold_crossV(df,5)\n",
    "bkt_parmtrs_4_all_skills=get_parmtrs_all_skills(df)\n",
    "# predicted_probability=get_predictions(df, bkt_parmtrs_4_all_skills)\n",
    "# C_RMSE_AUC_ACC=collapsed_metrics(predicted_probability)\n",
    "# NC_RMSE_AUC_ACC=non_collapsed_metrics(predicted_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "df_test=df[df['skill']==\"g1.m2.1.1.1\"]\n",
    "libObj=ct.cdll.LoadLibrary(\"/Users/nabilch/Desktop/bkt_c/find_par.so\")\n",
    "bkt_parmtrs=get_parmtrs_all_skills(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing whether removing rori students who did not attempt many qns (<10) makes any difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.student.value_counts().to_frame('count').reset_index()\n",
    "e=y[y['count']>9]\n",
    "e.columns=['a','b']\n",
    "df_test=df[df['student'].isin(e['a'])]\n",
    "# libObj=ct.cdll.LoadLibrary(\"/Users/nabilch/Desktop/bkt_c/find_par.so\")\n",
    "# c_rmse_auc_acc, nc_rmse_auc_acc, fold_c_rmse_auc_acc, fold_nc_rmse_auc_acc=ss_nfold_crossV(df_test,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkt_parmtrs_4_all_skills.to_csv(\"/Users/nabilch/Desktop/bkt_c/rori_rising_bkt_par.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
